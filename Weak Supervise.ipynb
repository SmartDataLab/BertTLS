{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/data1/su/app/text_forecast/data/datasets/nyt_new%20structure/')\n",
    "f1 = open('%20train.demo.json', mode='r')\n",
    "train_demo = json.load(f1)\n",
    "f2 = open('crisis_new.json', mode='r')\n",
    "crisis = json.load(f2)\n",
    "f3 = open('entities_new.json', mode='r')\n",
    "entities = json.load(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nyt:\n",
      "<class 'list'>\n",
      "110\n",
      "dict_keys(['name', 'src', 'page', 'page_mean', 'time', 'taxo'])\n",
      "crisis:\n",
      "<class 'list'>\n",
      "4\n",
      "dict_keys(['tgt5', 'tgt_date5', 'tgt6', 'tgt_date6', 'tgt7', 'tgt_date7', 'tgt8', 'tgt_date8', 'tgt9', 'tgt_date9', 'name', 'src', 'src_date'])\n",
      "entities:\n",
      "<class 'list'>\n",
      "46\n",
      "dict_keys(['tgt', 'tgt_date', 'name', 'src', 'src_date'])\n"
     ]
    }
   ],
   "source": [
    "print('nyt:')\n",
    "print(type(train_demo))\n",
    "print(len(train_demo))\n",
    "print(train_demo[0].keys())\n",
    "\n",
    "print('crisis:')\n",
    "print(type(crisis))\n",
    "print(len(crisis))\n",
    "print(crisis[0].keys())\n",
    "\n",
    "print('entities:')\n",
    "print(type(entities))\n",
    "print(len(entities))\n",
    "print(entities[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修改后的taxo距离计算函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taxostat_distance(timeline, depth) -> list:\n",
    "    \"\"\"\n",
    "    params:\n",
    "    timeline: json字典，.keys() = dict_keys(['name', 'src', 'page', 'page_mean', 'time', 'taxo'])\n",
    "    \n",
    "    depth: taxonomic classifier的最大追索深度,一般最大距离为depth-1\n",
    "\n",
    "    return: list,每一个时间节点距离基准taxonomic classifier的平均距离\n",
    "    \"\"\"\n",
    "    # 取出raw_taxostr\n",
    "    raw_taxostr_lst = timeline.get('taxo')\n",
    "    \n",
    "    # 划分taxostr\n",
    "    taxostr_lst = [raw_taxostr.split(\"|\") if type(raw_taxostr) == str else '' for raw_taxostr in raw_taxostr_lst ]\n",
    "    # taxostr分段\n",
    "    taxo_unit_lst = [[taxostr.split(\"/\") for taxostr in unit] if unit != '' else '' for unit in taxostr_lst ]\n",
    "\n",
    "    # 计算距离\n",
    "    # 以出现频次最高的taxo为基准\n",
    "    try:\n",
    "        base_taxo = (\n",
    "            pd.value_counts(\n",
    "                [\n",
    "                    \"/\".join(taxo[0 : min(depth, len(taxo))])\n",
    "                    for unit in taxo_unit_lst\n",
    "                    for taxo in unit\n",
    "                ]\n",
    "            )\n",
    "            .index[0]\n",
    "            .split(\"/\")\n",
    "        )\n",
    "    except:\n",
    "        return []\n",
    "    base_len = len(base_taxo)\n",
    "    # 计算每个时间节点内taxo的平均距离\n",
    "    taxo_distance_lst = []\n",
    "    for taxo_unit in taxo_unit_lst:\n",
    "        curr_scores = []\n",
    "        for taxo in taxo_unit:  # 计算每一个taxo距离base_taxo的距离\n",
    "            minus = 1\n",
    "            for i in range(min(base_len, len(taxo))):\n",
    "                if taxo[i] != base_taxo[i]:\n",
    "                    minus = 0\n",
    "                    break\n",
    "\n",
    "            score = depth - minus - i\n",
    "            curr_scores.append(score)\n",
    "\n",
    "        \n",
    "        #taxo_distance_lst.append(sum(curr_scores) * 1.0 / len(taxo_unit)\n",
    "        \n",
    "        if len(taxo_unit)!= 0:\n",
    "            taxo_distance_lst.append(sum(curr_scores) * 1.0 / len(taxo_unit))                         \n",
    "        else:\n",
    "            taxo_distance_lst.append(4.0) #taxo为空的距离取4.0\n",
    "\n",
    "    return taxo_distance_lst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算nyt的taxo距离(坑点:有小部分文章的taxo为空)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nato_and\n",
      "nan\n",
      "19980703T000000\n",
      "['Crews', 'from', 'six', 'NATO', 'ships', 'battle', 'in', 'tug', '-', 'of', '-', 'war', 'competition', 'next', 'to', 'Intrepid', 'Sea', '-', 'Air', '-', 'Space', 'Museum', ',', 'NYC', ';', 'photo', '(', 'S', ')']\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "#e.g\n",
    "print(train_demo[2].get('name'))\n",
    "print(train_demo[2].get('taxo')[449]) #这里为nan\n",
    "print(train_demo[2].get('time')[449])\n",
    "print(train_demo[2].get('src')[449])\n",
    "print(taxostat_distance(train_demo[2],4)[449])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Taxo_scores = [taxostat_distance(topic,4) for topic in train_demo]\n",
    "for i in range(0,len(train_demo)): train_demo[i]['taxo_score'] = Taxo_scores[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_n = open('%20train.demo+taxo.json', mode='w+')\n",
    "json.dump(train_demo ,f1_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 弱监督选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_supervision_selection(\n",
    "    doc_sent_list, doc_date_list, doc_page_list, doc_taxoscore_list, abstract_size = 8, page_weight = 1, taxo_weight = 10, use_date = True, date_size = 2\n",
    "):\n",
    "\n",
    "    origin_tuple = tuple(zip(range(len(doc_page_list)), doc_page_list, doc_taxoscore_list, doc_date_list))\n",
    "    \n",
    "    #oracle选取初步想法：\n",
    "    #1.先由page, taxo加权排序得到sorted_tuple(page越大越好，taxo越小越好), \n",
    "    #2.再取sorted_tuple中前n*abstract_size个(n待定)组成小集合selected_tuple\n",
    "    #3.再对小集合里所有的timeline组合取'date方差'最小的一组得到result_tuple\n",
    "    \n",
    "    sorted_tuple = sorted(\n",
    "        origin_tuple, key=lambda x: page_weight*x[1]+taxo_weight*(4-x[2]), reverse=True #w_page * page + w_taxo * (4-taxo)\n",
    "    )\n",
    "    \n",
    "    if len(sorted_tuple) <= abstract_size: use_date=False #当总文章数小于或等于时间线size数时，无法使用date筛选\n",
    "    \n",
    "    if use_date == True:\n",
    "        select_size = min(math.ceil(date_size*abstract_size), len(sorted_tuple))\n",
    "        selected_tuple = sorted_tuple[0:select_size-1]\n",
    "        min_Var_date = float(\"inf\")\n",
    "        for timeline in itertools.combinations(selected_tuple, abstract_size):\n",
    "            dates = list(zip(*timeline))[3]\n",
    "            datenum = [int(date[0:8]) for date in dates]\n",
    "            Var_date = np.var(datenum)\n",
    "            if Var_date < min_Var_date :\n",
    "                min_Var_date = Var_date\n",
    "                result_tuple = timeline\n",
    "    \n",
    "    else:\n",
    "        result_tuple = sorted_tuple[0:min(abstract_size, len(sorted_tuple))]\n",
    "        \n",
    "        \n",
    "    result_tuple = sorted(result_tuple,key = lambda x: x[3]) #result_tuple按时间顺序排序\n",
    "    \n",
    "    \n",
    "    abstract = []\n",
    "    abstract_date = []\n",
    "    oracle_ids = []\n",
    "    for i in range(len(result_tuple)):\n",
    "        idx = result_tuple[i][0]\n",
    "        oracle_ids.append(idx)\n",
    "        abstract.append(doc_sent_list[idx])\n",
    "        abstract_date.append(doc_date_list[idx])\n",
    "    \n",
    "    return abstract, abstract_date, oracle_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_lists = [topic.get('src') for topic in train_demo]\n",
    "date_lists = [topic.get('time') for topic in train_demo]\n",
    "page_lists = [topic.get('page') for topic in train_demo]\n",
    "taxo_lists = [topic.get('taxo_score') for topic in train_demo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数取默认值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20010225T000000',\n",
       " '20010620T000000',\n",
       " '20010914T000000',\n",
       " '20010914T000000',\n",
       " '20020329T000000',\n",
       " '20020329T000000',\n",
       " '20030608T000000',\n",
       " '20030921T000000']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_supervision_selection(sent_lists[0], date_lists[0], page_lists[0], taxo_lists[0], \n",
    "                           abstract_size=8)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不考虑date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19960817T000000',\n",
       " '19961212T000000',\n",
       " '19970928T000000',\n",
       " '19971214T000000',\n",
       " '19981213T000000',\n",
       " '20010620T000000',\n",
       " '20010914T000000',\n",
       " '20030921T000000']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_supervision_selection(sent_lists[0], date_lists[0], page_lists[0], taxo_lists[0], \n",
    "                           abstract_size=8,\n",
    "                          use_date=False)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不考虑page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19970928T000000',\n",
       " '19980802T000000',\n",
       " '20000402T000000',\n",
       " '20000730T000000',\n",
       " '20010620T000000',\n",
       " '20011006T000000',\n",
       " '20040613T000000',\n",
       " '20060423T000000']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_supervision_selection(sent_lists[0], date_lists[0], page_lists[0], taxo_lists[0],\n",
    "                           abstract_size=8,\n",
    "                           use_date=False,\n",
    "                           page_weight=0, \n",
    "                           )[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不考虑taxo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19960824T000000',\n",
       " '19960902T000000',\n",
       " '19961212T000000',\n",
       " '19970222T000000',\n",
       " '19971214T000000',\n",
       " '19981213T000000',\n",
       " '20030608T000000',\n",
       " '20030921T000000']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_supervision_selection(sent_lists[0], date_lists[0], page_lists[0], taxo_lists[0],\n",
    "                           abstract_size=8,\n",
    "                           use_date=False,\n",
    "                           taxo_weight=0, \n",
    "                           )[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#试一下数据能不能跑通\n",
    "for k in range(0,len(sent_lists)):\n",
    "    weak_supervision_selection(sent_lists[k], date_lists[k], page_lists[k], taxo_lists[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
