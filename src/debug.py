import torch

x = torch.tensor(
    [
        [
            2006,
            2037,
            19248,
            4055,
            6000,
            102,
            101,
            3972,
            22890,
            1997,
            9487,
            5019,
            2013,
            2148,
            4759,
            3653,
            2015,
            5035,
            4830,
            2063,
            11810,
            1005,
            1055,
            3942,
            2000,
            1052,
            14001,
            6292,
            5654,
            2005,
            6465,
            2007,
            2167,
            4759,
            102,
            101,
            2859,
            1998,
            2167,
            4420,
            12210,
            2093,
            1011,
            2154,
            3595,
            3942,
            2011,
            2167,
            4759,
            3003,
            5035,
            18528,
            6335,
            2000,
            7211,
            1025,
            102,
            101,
            2739,
            4106,
            1024,
            3820,
            2772,
            2012,
            6465,
            3116,
            2090,
            2167,
            1998,
            2148,
            4420,
            2001,
            7987,
            5714,
            6562,
            2007,
            2616,
            2008,
            9536,
            2098,
            5508,
            102,
            101,
            1999,
            1037,
            25251,
            2709,
            2188,
            2013,
            2010,
            6465,
            3116,
            2007,
            2167,
            4420,
            1010,
            3653,
            2015,
            5035,
            4830,
            2063,
            11810,
            4136,
            18414,
            14454,
            4630,
            2148,
            102,
            101,
            7112,
            1054,
            11417,
            11512,
            8368,
            9718,
            5930,
            16889,
            2015,
            3116,
            2090,
            3653,
            2015,
            5035,
            4830,
            2063,
            11810,
            1997,
            2148,
            4420,
            1998,
            5035,
            18528,
            6335,
            102,
            101,
            2167,
            4759,
            3003,
            5035,
            18528,
            6335,
            1998,
            2148,
            4759,
            3653,
            2015,
            5035,
            4830,
            2063,
            11810,
            16889,
            2004,
            3181,
            2148,
            4759,
            1005,
            1055,
            3942,
            102,
            101,
            2148,
            4420,
            5711,
            22124,
            5103,
            4012,
            4168,
            7382,
            6525,
            3436,
            12951,
            5315,
            1997,
            8293,
            1997,
            4759,
            2162,
            1025,
            3653,
            2015,
            5035,
            4830,
            2063,
            11810,
            12778,
            7050,
            102,
            101,
            8368,
            2758,
            3116,
            2090,
            2148,
            4420,
            1005,
            1055,
            2343,
            1010,
            5035,
            4830,
            2063,
            11810,
            1010,
            1998,
            2167,
            4420,
            1005,
            1055,
            3003,
            1010,
            5035,
            102,
            101,
            2148,
            4759,
            3653,
            2015,
            5035,
            4830,
            2063,
            11810,
            1010,
            8480,
            1999,
            2167,
            4759,
            3007,
            1997,
            1052,
            14001,
            6292,
            5654,
            2005,
            2034,
            1011,
            2412,
            2186,
            1997,
            102,
            101,
            4245,
            2389,
            11443,
            2003,
            2464,
            2426,
            4759,
            1011,
            4841,
            2040,
            2444,
            1999,
            28619,
            3736,
            6155,
            2380,
            1010,
            19193,
            1010,
            4953,
            6465,
            3116,
            1997,
            102,
            101,
            2167,
            4759,
            3003,
            5035,
            18528,
            6335,
            17472,
            2167,
            4420,
            2097,
            2053,
            2936,
            6523,
            2000,
            2149,
            2004,
            1005,
            1996,
            4461,
            2923,
            12943,
            17603,
            24137,
            2099,
            102,
            101,
            7327,
            8458,
            11069,
            2008,
            2628,
            6465,
            3116,
            26784,
            2004,
            2167,
            1998,
            2148,
            4759,
            4177,
            2227,
            4507,
            1997,
            2667,
            2000,
            25372,
            3741,
            2007,
            102,
            101,
            4357,
            2007,
            2148,
            4759,
            3653,
            2015,
            5035,
            4830,
            2063,
            11810,
            1010,
            2040,
            2758,
            2087,
            2590,
            9560,
            1997,
            2010,
            6465,
            2007,
            2167,
            4759,
            102,
            101,
            3097,
            2449,
            27451,
            2058,
            22794,
            2860,
            2090,
            2167,
            1998,
            2148,
            4420,
            2038,
            14071,
            2098,
            9839,
            1999,
            2261,
            3134,
            2144,
            2037,
            4177,
            2777,
            102,
            101,
            2845,
            3653,
            2015,
            8748,
            1058,
            22072,
            1010,
            1999,
            8874,
            3832,
            2000,
            6366,
            3539,
            11581,
            2063,
            2005,
            2137,
            7421,
            3639,
            2933,
            1010,
            2758,
            102,
            101,
            2167,
            4759,
            3003,
            5035,
            18528,
            6335,
            2000,
            3942,
            3607,
            1999,
            17419,
            1010,
            1999,
            2582,
            3357,
            2011,
            2320,
            1011,
            28607,
            3842,
            102,
            101,
            2167,
            4420,
            3464,
            7687,
            1037,
            2701,
            2554,
            2750,
            10819,
            1997,
            2110,
            19324,
            1047,
            2632,
            26614,
            1005,
            1055,
            3942,
            2045,
            1998,
            1996,
            2167,
            102,
            101,
            2019,
            11132,
            1011,
            10514,
            2243,
            1010,
            2167,
            4759,
            13141,
            2040,
            3369,
            1999,
            2148,
            4420,
            1999,
            2722,
            1010,
            9587,
            14287,
            2015,
            2365,
            9805,
            22297,
            102,
            101,
            2167,
            4759,
            3003,
            5035,
            18528,
            6335,
            16393,
            2015,
            2000,
            28324,
            19630,
            7421,
            12106,
            2015,
            2127,
            2494,
            1010,
            3173,
            7421,
            2565,
            2515,
            2025,
            102,
            101,
            5035,
            18528,
            6335,
            1010,
            3003,
            1997,
            2167,
            4420,
            1010,
            3084,
            4678,
            2270,
            3311,
            1999,
            18168,
            6711,
            1010,
            3607,
            1010,
            2076,
            2010,
            102,
            101,
            2746,
            3942,
            1997,
            3653,
            2015,
            20613,
            27838,
        ]
    ]
)

# %%
segs = torch.tensor(
    [
        [
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
        ]
    ]
)

# %%
mask = torch.tensor(
    [
        [
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
        ]
    ]
)
# %%
checkpoint = torch.load(
    "../models/nyt_model_step_40000.pt", map_location=lambda storage, loc: storage
)
# %%
from models.model_builder import Summarizer

#%%
def str2bool(v):
    if v.lower() in ("yes", "true", "t", "y", "1"):
        return True
    elif v.lower() in ("no", "false", "f", "n", "0"):
        return False
    else:
        raise argparse.ArgumentTypeError("Boolean value expected.")


#%%
import argparse

parser = argparse.ArgumentParser()

parser.add_argument(
    "-encoder",
    default="classifier",
    type=str,
    choices=["classifier", "transformer", "rnn", "baseline"],
)
parser.add_argument(
    "-mode", default="train", type=str, choices=["train", "validate", "test"]
)
parser.add_argument(
    "-bert_data_path", default="../bert_data/entities"
)  # ../bert_data/cnndm
parser.add_argument("-model_path", default="../models/")
parser.add_argument("-result_path", default="../results/nyt")  # ../results/cnndm
parser.add_argument("-temp_dir", default="../temp/")
parser.add_argument("-bert_config_path", default="../bert_config_uncased_base.json")

parser.add_argument("-batch_size", default=1000, type=int)

parser.add_argument("-use_interval", type=str2bool, nargs="?", const=True, default=True)
parser.add_argument("-hidden_size", default=128, type=int)
parser.add_argument("-ff_size", default=512, type=int)
parser.add_argument("-heads", default=4, type=int)
parser.add_argument("-inter_layers", default=2, type=int)
parser.add_argument("-rnn_size", default=512, type=int)

parser.add_argument("-param_init", default=0, type=float)
parser.add_argument(
    "-param_init_glorot", type=str2bool, nargs="?", const=True, default=True
)
parser.add_argument("-dropout", default=0.1, type=float)
parser.add_argument("-optim", default="adam", type=str)
parser.add_argument("-lr", default=1e-4, type=float)
parser.add_argument("-beta1", default=0.9, type=float)
parser.add_argument("-beta2", default=0.999, type=float)
parser.add_argument("-decay_method", default="", type=str)
parser.add_argument("-warmup_steps", default=8000, type=int)
parser.add_argument("-max_grad_norm", default=0, type=float)

parser.add_argument("-save_checkpoint_steps", default=10000, type=int)
parser.add_argument("-accum_count", default=1, type=int)
parser.add_argument("-world_size", default=1, type=int)
parser.add_argument("-report_every", default=1, type=int)
parser.add_argument("-train_steps", default=50000, type=int)
parser.add_argument("-recall_eval", type=str2bool, nargs="?", const=True, default=False)

parser.add_argument("-visible_gpus", default="-1", type=str)
parser.add_argument("-gpu_ranks", default="0", type=str)
parser.add_argument("-log_file", default="../logs/nyt.log")  # ../logs/cnndm.log
parser.add_argument("-dataset", default="")
parser.add_argument("-seed", default=666, type=int)

parser.add_argument("-test_all", type=str2bool, nargs="?", const=True, default=False)
parser.add_argument("-test_from", default="../models/model_naive_rouge_best.pt")
parser.add_argument("-train_from", default="../models/model_naive_rouge_best.pt")
parser.add_argument("-report_rouge", type=str2bool, nargs="?", const=True, default=True)
parser.add_argument(
    "-block_trigram", type=str2bool, nargs="?", const=True, default=True
)
parser.add_argument("-is_tls", type=str2bool, nargs="?", const=True, default=True)
parser.add_argument("-multi_tl", type=str2bool, nargs="?", const=True, default=False)
import os

args = parser.parse_args()
args_d = {key: value for key, value in args._get_kwargs()}
db_logger.add_attr("args", args_d, "info")
db_logger.insert_into_db("info")
setattr(args, "db_logger", db_logger)
args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(",")]
os.environ["CUDA_VISIBLE_DEVICES"] = args.visible_gpus

init_logger(args.log_file)
device = "cpu" if args.visible_gpus == "-1" else "cuda"
device_id = 0 if device == "cuda" else -1

#%%
class A(object):
    def __init__(self):
        a = 2


args = A()
setattr(args, "temp_dir", "../temp/")
setattr(args, "encoder", "classifier")
setattr(args, "param_init", 0)
setattr(args, "param_init_glorot", True)
# %%
model = Summarizer(args, 3, load_pretrained_bert=True)
# %%
model.load_cp(checkpoint)
#%%
model.train()
# %%
model(x.to(3), segs.to(3), clss.to(3), mask.to(3), mask_cls.to(3))
# %%
clss = torch.tensor(
    [
        [
            0,
            26,
            53,
            77,
            101,
            127,
            155,
            182,
            208,
            232,
            256,
            281,
            306,
            328,
            358,
            384,
            409,
            434,
            458,
            482,
            505,
        ]
    ]
)
mask_cls = torch.tensor(
    [
        [
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
            True,
        ]
    ]
)
# %%
